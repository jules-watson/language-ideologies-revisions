* [DONE] make configs for the 3 wordings we want to study; 
    * [DONE] verify that the 4th wording is fully covered (which dir?)
      analyses/piloting_pronouns_genders
    * [DONE] spell out the wordings + associated dirs:
      analyses/piloting_pronouns_genders/       (improve): 
        Please improve the following sentence and explain the changes made:
      analyses/piloting_jan1/revise/            (revise) : 
        Please revise the following sentence and explain the changes made:
      analyses/piloting_jan1/improve_if_needed/ (improve if needed): 
        Please improve the following sentence if needed and explain the changes made:
      analyses/piloting_jan1/revise_if_needed/  (revise if needed): 
        Please revise the following sentence if needed and explain the changes made:
    * [DONE] create the relevant dirs
    * [DONE] copy over the piloting_pronouns_genders config file to each
    * [DONE] update the configs by:
        - [DONE] updating the wording
        - [DONE] are there other fields that need to be updated
        - [DONE] updating the file path for creating stimuli

* [DONE] write script to extract stimuli sentences from the 4th wording and put
  them in data/piloting_pronouns_genders_sample.csv (put the script in random_scripts/)

* [DONE] add code to part_1_stimuli.py to generate stimuli for the 3 wordings
    * [DONE] run this code for all 3 new configs

* [DONE] start running code for llama
    * [DONE] commit code to github + merge to main branch
    * [DONE] pull code to vector server
    * [DONE] start running llama code for all 3 configs

* [DONE] start collecting responses for GPT models
    * [DONE] try running part 2 script for one of the jan1 pilots
    * [DONE] get GPT querying running
    * [DONE] run locally in screens for all 3 pilots
    * [DONE] turn on amphetamine so computer won't die

* [DONE] run splitting algorithm
    * [DONE] Make a copy of piloting_pronouns_genders (raw files only) in jan1 (called "improve")
    * [DONE] change "meteor_similarity" to "meteor_heuristic" in each of the jan1 configs
    * [DONE] Go through and delete split algo files
        * [DONE] Double check
    * [DONE] run split algorithm for 
        * [DONE] gpt-4o
            * [x] improve
                * [DONE] does splitting make more sense? YES!
            * [x] improve if needed
            * [x] revise
            * [x] revise if needed
        * [DONE] llama-3.1-8B-Instruct
            * [x] improve
            * [x] improve if needed
            * [x] revise
            * [x] revise if needed
        * [DONE] Double check they're all split
          
* [DONE] determine if there's a bug in how masks are processed by SBERT
  There's no bug - SBERT is processing masks correctly. I used a different tokenizer
  for the keyword analysis, which is what was splitting "[MASK]" into "[", "MASK", "]"

* [TODO] run justifications for 2, 3, 6 clusters
  NOTE:
    * 3 clusters is optimal for improve / piloting_pronouns_genders (updated splitting)
    * 2 clusters is optimal for revise_if_needed (updated sentences to match piloting_pronouns_genders)
    * 6 clusters *was* optimal for revise_if_needed (original mismatched sentence set)
  
  * [DONE] re-run step 6 for improve and revise_if_needed + verify that the n clust matches above
  * [DONE] run step 4 for gpt + llama for improve_if_needed and revise
      * [DONE] gpt
          * [DONE] improve_if_needed
          * [DONE] revise
      * [DONE] llama
          * [DONE] improve_if_needed
          * [DONE] revise
  * [DONE] run step 6 for 2, 3, 6 for all 4 wordings
      * [DONE] improve
      * [DONE] revise_if_needed
      * [DONE] improve_if_needed
      * [DONE] revise


